{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd70bead",
   "metadata": {},
   "source": [
    "# 1. Prepare data\n",
    "\n",
    "For each voxel, the **BraInCoRL** model takes image - beta value pairs as in-context examples, and predicts the beta values of query images.\n",
    "\n",
    "First, load the in-context image embeddings, in-context beta values and query images from numpy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3dd4825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ic_img.shape (10, 100, 512)\n",
      "ic_beta.shape (10, 100)\n",
      "query_img.shape (10, 20, 512)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "data_dir = Path('./data')\n",
    "\n",
    "# load in-context image embeddings\n",
    "ic_img_path = data_dir / 'sample_in_ctx_imgs.npz'\n",
    "ic_img = np.load(ic_img_path)['arr_0']\n",
    "print('ic_img.shape', ic_img.shape)     # (10 voxels, 100 in-context images, 512 embed dim)\n",
    "\n",
    "# load in-context beta values\n",
    "ic_beta_path = data_dir / 'sample_in_ctx_betas.npz'\n",
    "ic_beta = np.load(ic_beta_path)['arr_0']\n",
    "print('ic_beta.shape', ic_beta.shape)     # (10 voxels, 100 in-context images, )\n",
    "\n",
    "# load query image embeddings\n",
    "query_img_path = data_dir / 'sample_query_imgs.npz'\n",
    "query_img = np.load(query_img_path)['arr_0']\n",
    "print('query_img.shape', query_img.shape)     # (10 voxels, 20 query images, 512 embed dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d312d",
   "metadata": {},
   "source": [
    "# 2. Load pretrained model\n",
    "\n",
    "The model script is in `./scripts/model.py` and the checkpoints should be placed in `./scripts/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b872deca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "HyperweightsPredictorModel INITIALIZATION PARAMETERS\n",
      "==================================================\n",
      "embed_dim: 512\n",
      "internal_emb_dim: 560\n",
      "num_tsfm_layers: 20\n",
      "tsfm_hidden_dim: 2048\n",
      "num_reg_tok: 4\n",
      "num_heads: 10\n",
      "num_early_lyr: 1\n",
      "num_w_pred_layers: 1\n",
      "early_hidden_dim: 1120\n",
      "w_pred_hidden_dim: 1120\n",
      "dropout: 0\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./scripts')\n",
    "from model import HyperweightsPredictorModel    # the BraInCoRL model\n",
    "import torch\n",
    "\n",
    "backbone_type = 'CLIP'  # or change to other image embedding backbones\n",
    "predict_subj = 1\n",
    "device = f'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = HyperweightsPredictorModel(backbone_type=backbone_type).to(device)\n",
    "\n",
    "# load weights\n",
    "model_ckpt_path = f'./checkpoints/{backbone_type}_subj{predict_subj}.pth'\n",
    "checkpoint = torch.load(model_ckpt_path, weights_only=True)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085ec4d",
   "metadata": {},
   "source": [
    "# 3. Inference\n",
    "\n",
    "Feed the loaded data to the model to get the inference results.\n",
    "\n",
    "The model output result consists of two parts:\n",
    "\n",
    "1. The predicted beta values of the query images\n",
    "\n",
    "2. the predicted voxelwise mapping weights, so that  \n",
    "`weights(image_embed) == predicted_beta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4651a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_betas.shape torch.Size([10, 20])\n",
      "predicted_weights.shape torch.Size([10, 513])\n"
     ]
    }
   ],
   "source": [
    "ic_img = torch.from_numpy(ic_img).to(device).float()\n",
    "ic_beta = torch.from_numpy(ic_beta).to(device).float()\n",
    "query_img = torch.from_numpy(query_img).to(device).float()\n",
    "\n",
    "predicted_betas, predicted_weights = model(ic_img, ic_beta, query_img)\n",
    "\n",
    "print('predicted_betas.shape', predicted_betas.shape)       # (10 voxels, 20 query images)\n",
    "print('predicted_weights.shape', predicted_weights.shape)   # (10 voxels, 513 projection weights dimension)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
